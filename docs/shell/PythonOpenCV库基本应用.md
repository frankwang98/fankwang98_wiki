







> 
> ğŸ˜*â˜…,Â°*:.â˜†(ï¿£â–½ï¿£)/$:*.Â°â˜…* ğŸ˜  
>  è¿™ç¯‡æ–‡ç« ä¸»è¦ä»‹ç»OpenCVåº“åŸºæœ¬åº”ç”¨ã€‚  
>  **å­¦å…¶æ‰€ç”¨ï¼Œç”¨å…¶æ‰€å­¦ã€‚â€”â€”æ¢å¯è¶…**  
>  æ¬¢è¿æ¥åˆ°æˆ‘çš„åšå®¢ï¼Œä¸€èµ·å­¦ä¹ ï¼Œå…±åŒè¿›æ­¥ã€‚  
>  å–œæ¬¢çš„æœ‹å‹å¯ä»¥å…³æ³¨ä¸€ä¸‹ï¼Œä¸‹æ¬¡æ›´æ–°ä¸è¿·è·¯ğŸ¥
> 
> 
> 




#### æ–‡ç« ç›®å½•


* + [:smirk:1. opencv-pythonä»‹ç»](#smirk1_opencvpython_7)
	+ [:blush:2. ç¯å¢ƒå®‰è£…ä¸é…ç½®](#blush2__25)
	+ [:satisfied:3. åº”ç”¨ç¤ºä¾‹](#satisfied3__37)




### ğŸ˜1. opencv-pythonä»‹ç»


OpenCVï¼ˆOpen Source Computer Vision Libraryï¼‰æ˜¯ä¸€ä¸ªå¹¿æ³›ä½¿ç”¨çš„å¼€æºè®¡ç®—æœºè§†è§‰åº“ï¼Œå®ƒæä¾›äº†ç”¨äºå¤„ç†å›¾åƒå’Œè§†é¢‘çš„å„ç§åŠŸèƒ½å’Œç®—æ³•ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å¸¸è§çš„åŠŸèƒ½å’Œåº”ç”¨ï¼š



> 
> 1.å›¾åƒå’Œè§†é¢‘çš„è¯»å–å’Œå†™å…¥ï¼šOpenCV å¯ä»¥è¯»å–å’Œå†™å…¥å„ç§å›¾åƒå’Œè§†é¢‘æ ¼å¼ï¼ŒåŒ…æ‹¬å¸¸è§çš„ JPEGã€PNGã€BMP å’Œè§†é¢‘æ–‡ä»¶ã€‚
> 
> 
> 



> 
> 2.å›¾åƒå¤„ç†å’Œæ“ä½œï¼šOpenCV æä¾›äº†è®¸å¤šå›¾åƒå¤„ç†å’Œæ“ä½œåŠŸèƒ½ï¼Œå¦‚è°ƒæ•´å¤§å°ã€è£å‰ªã€æ—‹è½¬ã€ç¿»è½¬ã€å¹³ç§»ã€æ»¤æ³¢ã€è¾¹ç¼˜æ£€æµ‹ã€ç›´æ–¹å›¾å‡è¡¡åŒ–ç­‰ã€‚
> 
> 
> 



> 
> 3.ç‰¹å¾æ£€æµ‹å’Œæè¿°ï¼šOpenCV æä¾›å„ç§ç‰¹å¾æ£€æµ‹å’Œæè¿°ç®—æ³•ï¼ŒåŒ…æ‹¬å…³é”®ç‚¹æ£€æµ‹ï¼ˆå¦‚ Harris è§’ç‚¹æ£€æµ‹ã€FAST ç‰¹å¾æ£€æµ‹ç­‰ï¼‰ã€ç‰¹å¾æè¿°ï¼ˆå¦‚ SIFTã€SURFã€ORBã€BRISK ç®—æ³•ç­‰ï¼‰å’Œç‰¹å¾åŒ¹é…ã€‚
> 
> 
> 



> 
> 4.ç›®æ ‡æ£€æµ‹å’Œè·Ÿè¸ªï¼šOpenCV æ”¯æŒå¤šç§ç›®æ ‡æ£€æµ‹å’Œè·Ÿè¸ªç®—æ³•ï¼Œå¦‚ Haar ç‰¹å¾åˆ†ç±»å™¨ã€HOG+SVMã€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚ TensorFlowã€PyTorchï¼‰é›†æˆç­‰ã€‚
> 
> 
> 



> 
> 5.å›¾åƒåˆ†å‰²å’Œè½®å»“æå–ï¼šOpenCV æä¾›äº†å„ç§å›¾åƒåˆ†å‰²ç®—æ³•ï¼Œå¦‚åŸºäºé˜ˆå€¼çš„æ–¹æ³•ã€åŸºäºè¾¹ç¼˜çš„æ–¹æ³•ï¼ˆå¦‚ Canny è¾¹ç¼˜æ£€æµ‹ï¼‰ä»¥åŠæ›´é«˜çº§çš„åˆ†å‰²ç®—æ³•ï¼ˆå¦‚ GrabCutã€åˆ†æ°´å²­ç®—æ³•ç­‰ï¼‰ã€‚
> 
> 
> 



> 
> 6.ç›¸æœºæ ‡å®šå’Œå‡ ä½•æ ¡æ­£ï¼šOpenCV æ”¯æŒç›¸æœºæ ‡å®šå’Œå‡ ä½•æ ¡æ­£ï¼Œå¸®åŠ©æ¶ˆé™¤å›¾åƒä¸­çš„ç•¸å˜ï¼Œå¹¶æ¢å¤çœŸå®ä¸–ç•Œä¸­çš„å‡ ä½•ä¿¡æ¯ã€‚
> 
> 
> 



> 
> 7.è®¡ç®—æœºè§†è§‰åº”ç”¨ï¼šOpenCV åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸæœ‰å¹¿æ³›çš„åº”ç”¨ï¼ŒåŒ…æ‹¬äººè„¸æ£€æµ‹å’Œè¯†åˆ«ã€ç›®æ ‡è·Ÿè¸ªã€å›¾åƒæ‹¼æ¥ã€å›¾åƒå¢å¼ºã€å®æ—¶å›¾åƒå¤„ç†ã€è™šæ‹Ÿå’Œå¢å¼ºç°å®ç­‰ã€‚
> 
> 
> 


OpenCV æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€ï¼ŒåŒ…æ‹¬ C++ã€Pythonã€Java ç­‰ã€‚åœ¨ Python ä¸­ä½¿ç”¨ OpenCVï¼Œå¯ä»¥é€šè¿‡å®‰è£…ç›¸åº”çš„ Python åŒ… `opencv-python` æ¥ä½¿ç”¨ã€‚


### ğŸ˜Š2. ç¯å¢ƒå®‰è£…ä¸é…ç½®


pipåŒ…å®‰è£…æ¯”è¾ƒç®€å•ï¼Œç”¨ `pip install opencv-python` å³å¯ï¼Œä½†æœ‰æ—¶å› ä¸ºç‰ˆæœ¬çš„ä¸åŒï¼Œå®‰è£…ä¼šå‡ºé”™ï¼Œæˆ‘çš„é”™è¯¯æ˜¯ï¼š



```
ERROR: Could not find a version that satisfies the requirement opencv-python (from versions: none)
ERROR: No matching distribution found for opencv-python

```

ç„¶åæ˜¯é€šè¿‡ä¸‹è½½ç¦»çº¿çš„whlæ–‡ä»¶å®‰è£…å¯ä»¥ï¼ŒwhlåŒ…åœ°å€ï¼š`https://pypi.tuna.tsinghua.edu.cn/simple/opencv-python/`


ç„¶åå®‰è£…ï¼š`pip install xxx.whl`


æœ€åè¯´ä¸€å¥ï¼Œpythonçš„ç”Ÿæ€å¾ˆå¼ºï¼Œå°è£…çš„åº“å¤ªå¥½ç”¨äº†ï¼Œè‹¥æ˜¯é…c++çš„opencvç¯å¢ƒï¼Œå¾—éº»çƒ¦10å€ã€‚


### ğŸ˜†3. åº”ç”¨ç¤ºä¾‹


äººè„¸è¯†åˆ«ç¤ºä¾‹ï¼š



```
import cv2

# åŠ è½½äººè„¸è¯†åˆ«çš„æ¨¡å‹æ–‡ä»¶
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade\_frontalface\_default.xml')

# æ‰“å¼€æ‘„åƒå¤´
cap = cv2.VideoCapture(0)

while True:
    # è¯»å–æ‘„åƒå¤´å¸§
    ret, frame = cap.read()

    # å°†å¸§è½¬æ¢ä¸ºç°åº¦å›¾åƒ
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # åœ¨ç°åº¦å›¾åƒä¸Šè¿›è¡Œäººè„¸æ£€æµ‹
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

    # åœ¨æ£€æµ‹åˆ°çš„æ¯ä¸ªäººè„¸å‘¨å›´ç”»ä¸€ä¸ªçŸ©å½¢æ¡†
    for (x, y, w, h) in faces:
        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 3)

    # æ˜¾ç¤ºç»“æœå¸§
    cv2.imshow('Face Detection', frame)

    # æŒ‰ä¸‹ 'q' é”®é€€å‡ºå¾ªç¯
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# é‡Šæ”¾èµ„æº
cap.release()
cv2.destroyAllWindows()


```

çœ¼åŠ¨æ£€æµ‹ï¼š



```
import cv2

# åŠ è½½çœ¼ç›æ£€æµ‹å™¨
# xmlä¸‹è½½ï¼šhttps://github.com/anaustinbeing/haar-cascade-files/blob/master/haarcascade\_eye.xml
eye_cascade = cv2.CascadeClassifier('haarcascade\_eye.xml')

# æ‰“å¼€æ‘„åƒå¤´
cap = cv2.VideoCapture(0)

# å®šä¹‰ç”¨äºè·Ÿè¸ªçœ¼ç›ä½ç½®çš„å˜é‡
prev_eye_x = 0
prev_eye_y = 0

while True:
    # ä»æ‘„åƒå¤´æ•è·è§†é¢‘å¸§
    ret, frame = cap.read()

    # å°†å›¾åƒè½¬æ¢ä¸ºç°åº¦å›¾åƒ
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    # æ£€æµ‹çœ¼ç›
    eyes = eye_cascade.detectMultiScale(gray, 1.3, 5)

    # éå†æ£€æµ‹åˆ°çš„çœ¼ç›
    for (x, y, w, h) in eyes:
        # ç»˜åˆ¶çŸ©å½¢æ¡†æ˜¾ç¤ºçœ¼ç›ä½ç½®
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)

        # è®¡ç®—çœ¼ç›çš„ä¸­å¿ƒåæ ‡
        eye_x = x + w // 2
        eye_y = y + h // 2

        # è®¡ç®—çœ¼ç›çš„è¿åŠ¨
        eye_dx = eye_x - prev_eye_x
        eye_dy = eye_y - prev_eye_y

        # æ›´æ–°ä¹‹å‰çš„çœ¼ç›ä½ç½®
        prev_eye_x = eye_x
        prev_eye_y = eye_y

        # åœ¨å›¾åƒä¸Šç»˜åˆ¶çœ¼ç›è¿åŠ¨å‘é‡
        cv2.arrowedLine(frame, (eye_x, eye_y), (eye_x + eye_dx, eye_y + eye_dy), (0, 0, 255), 2)

    # æ˜¾ç¤ºè§†é¢‘å¸§
    cv2.imshow('Eye Tracking', frame)

    # æŒ‰ä¸‹ "q" é”®é€€å‡ºå¾ªç¯
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# é‡Šæ”¾æ‘„åƒå¤´èµ„æº
cap.release()
cv2.destroyAllWindows()

```

æ‰‹åŠ¿æ§åˆ¶ç”µè„‘éŸ³é‡ç¤ºä¾‹ï¼š



```
import cv2
import mediapipe as mp
from ctypes import cast, POINTER
from comtypes import CLSCTX_ALL
from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume
import time
import math
import numpy as np

# æ‰‹åŠ¿æ§åˆ¶éŸ³é‡
class HandControlVolume:
    def \_\_init\_\_(self):
        # åˆå§‹åŒ–medialpipe
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        self.mp_hands = mp.solutions.hands

        # è·å–ç”µè„‘éŸ³é‡èŒƒå›´
        devices = AudioUtilities.GetSpeakers()
        interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)
        self.volume = cast(interface, POINTER(IAudioEndpointVolume))
        self.volume.SetMute(0, None)
        self.volume_range = self.volume.GetVolumeRange()

    # ä¸»å‡½æ•°
    def recognize(self):
        # è®¡ç®—åˆ·æ–°ç‡
        fpsTime = time.time()

        # OpenCVè¯»å–è§†é¢‘æµ(0)
        cap = cv2.VideoCapture(0)
        # è§†é¢‘åˆ†è¾¨ç‡
        resize_w = 640
        resize_h = 480

        # ç”»é¢æ˜¾ç¤ºåˆå§‹åŒ–å‚æ•°
        rect_height = 0
        rect_percent_text = 0

        with self.mp_hands.Hands(min_detection_confidence=0.7,
                                 min_tracking_confidence=0.5,
                                 max_num_hands=2) as hands:
            while cap.isOpened():
                success, image = cap.read()
                image = cv2.resize(image, (resize_w, resize_h))

                if not success:
                    print("ç©ºå¸§.")
                    continue

                # æé«˜æ€§èƒ½
                image.flags.writeable = True
                # è½¬ä¸ºRGB
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                # é•œåƒ
                image = cv2.flip(image, 1)
                # mediapipeæ¨¡å‹å¤„ç†
                results = hands.process(image)

                image.flags.writeable = True
                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
                # åˆ¤æ–­æ˜¯å¦æœ‰æ‰‹æŒ
                if results.multi_hand_landmarks:
                    # éå†æ¯ä¸ªæ‰‹æŒ
                    for hand_landmarks in results.multi_hand_landmarks:
                        # åœ¨ç”»é¢æ ‡æ³¨æ‰‹æŒ‡
                        self.mp_drawing.draw_landmarks(
                            image,
                            hand_landmarks,
                            self.mp_hands.HAND_CONNECTIONS,
                            self.mp_drawing_styles.get_default_hand_landmarks_style(),
                            self.mp_drawing_styles.get_default_hand_connections_style())

                        # è§£ææ‰‹æŒ‡ï¼Œå­˜å…¥å„ä¸ªæ‰‹æŒ‡åæ ‡
                        landmark_list = []
                        for landmark_id, finger_axis in enumerate(
                                hand_landmarks.landmark):
                            landmark_list.append([
                                landmark_id, finger_axis.x, finger_axis.y,
                                finger_axis.z
                            ])
                        if landmark_list:
                            # è·å–å¤§æ‹‡æŒ‡æŒ‡å°–åæ ‡
                            thumb_finger_tip = landmark_list[4]
                            thumb_finger_tip_x = math.ceil(thumb_finger_tip[1] \* resize_w)
                            thumb_finger_tip_y = math.ceil(thumb_finger_tip[2] \* resize_h)
                            # è·å–é£ŸæŒ‡æŒ‡å°–åæ ‡
                            index_finger_tip = landmark_list[8]
                            index_finger_tip_x = math.ceil(index_finger_tip[1] \* resize_w)
                            index_finger_tip_y = math.ceil(index_finger_tip[2] \* resize_h)
                            # ä¸­é—´ç‚¹
                            finger_middle_point = (thumb_finger_tip_x + index_finger_tip_x) // 2, (
                                    thumb_finger_tip_y + index_finger_tip_y) // 2
                            # print(thumb\_finger\_tip\_x)
                            thumb_finger_point = (thumb_finger_tip_x, thumb_finger_tip_y)
                            index_finger_point = (index_finger_tip_x, index_finger_tip_y)
                            # ç”»æŒ‡å°–2ç‚¹
                            image = cv2.circle(image, thumb_finger_point, 10, (255, 0, 255), -1)
                            image = cv2.circle(image, index_finger_point, 10, (255, 0, 255), -1)
                            image = cv2.circle(image, finger_middle_point, 10, (255, 0, 255), -1)
                            # ç”»2ç‚¹è¿çº¿
                            image = cv2.line(image, thumb_finger_point, index_finger_point, (255, 0, 255), 5)
                            # å‹¾è‚¡å®šç†è®¡ç®—é•¿åº¦
                            line_len = math.hypot((index_finger_tip_x - thumb_finger_tip_x),
                                                  (index_finger_tip_y - thumb_finger_tip_y))

                            # è·å–ç”µè„‘æœ€å¤§æœ€å°éŸ³é‡
                            min_volume = self.volume_range[0]
                            max_volume = self.volume_range[1]
                            # å°†æŒ‡å°–é•¿åº¦æ˜ å°„åˆ°éŸ³é‡ä¸Š
                            vol = np.interp(line_len, [50, 300], [min_volume, max_volume])
                            # å°†æŒ‡å°–é•¿åº¦æ˜ å°„åˆ°çŸ©å½¢æ˜¾ç¤ºä¸Š
                            rect_height = np.interp(line_len, [50, 300], [0, 200])
                            rect_percent_text = np.interp(line_len, [50, 300], [0, 100])

                            # è®¾ç½®ç”µè„‘éŸ³é‡
                            self.volume.SetMasterVolumeLevel(vol, None)

                # æ˜¾ç¤ºçŸ©å½¢
                cv2.putText(image, str(math.ceil(rect_percent_text)) + "%", (10, 350),
                            cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)
                image = cv2.rectangle(image, (30, 100), (70, 300), (255, 0, 0), 3)
                image = cv2.rectangle(image, (30, math.ceil(300 - rect_height)), (70, 300), (255, 0, 0), -1)

                # æ˜¾ç¤ºåˆ·æ–°ç‡FPS
                cTime = time.time()
                fps_text = 1 / (cTime - fpsTime)
                fpsTime = cTime
                cv2.putText(image, "FPS: " + str(int(fps_text)), (10, 70),
                            cv2.FONT_HERSHEY_PLAIN, 3, (255, 0, 0), 3)
                # æ˜¾ç¤ºç”»é¢
                cv2.imshow('HandControlVolume', image)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
            cap.release()

# åˆå§‹åŒ–ä¸è¯†åˆ«è°ƒç”¨
control = HandControlVolume()
control.recognize()

```

yoloç›®æ ‡æ£€æµ‹ç¤ºä¾‹ï¼š



```
import cv2

# åŠ è½½ç›®æ ‡æ£€æµ‹å™¨
net = cv2.dnn.readNetFromDarknet("yolov3.cfg", "yolov3.weights")  # æ›¿æ¢ä¸ºå®é™…çš„æ¨¡å‹å’Œæƒé‡æ–‡ä»¶è·¯å¾„
layers_names = net.getLayerNames()
output_layers = [layers_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]

# åŠ è½½å›¾åƒ
image = cv2.imread("image.jpg")  # æ›¿æ¢ä¸ºå®é™…çš„å›¾åƒæ–‡ä»¶è·¯å¾„

# å¯¹å›¾åƒè¿›è¡Œé¢„å¤„ç†
blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
net.setInput(blob)
outs = net.forward(output_layers)

# è§£æè¾“å‡ºå¹¶ç»˜åˆ¶è¾¹ç•Œæ¡†
class_ids = []
confidences = []
boxes = []
width = image.shape[1]
height = image.shape[0]

for out in outs:
    for detection in out:
        scores = detection[5:]
        class_id = np.argmax(scores)
        confidence = scores[class_id]
        if confidence > 0.5:  # è®¾ç½®ç½®ä¿¡åº¦é˜ˆå€¼
            center_x = int(detection[0] \* width)
            center_y = int(detection[1] \* height)
            w = int(detection[2] \* width)
            h = int(detection[3] \* height)
            x = int(center_x - w / 2)
            y = int(center_y - h / 2)

            boxes.append([x, y, w, h])
            confidences.append(float(confidence))
            class_ids.append(class_id)

indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)  # è®¾ç½®éæœ€å¤§æŠ‘åˆ¶é˜ˆå€¼

font = cv2.FONT_HERSHEY_SIMPLEX
for i in range(len(boxes)):
    if i in indexes:
        x, y, w, h = boxes[i]
        label = str(class_ids[i])
        confidence = confidences[i]
        color = (0, 0, 255)
        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)
        cv2.putText(image, label, (x, y - 10), font, 0.5, color, 2)

# æ˜¾ç¤ºç»“æœå›¾åƒ
cv2.imshow("Object Detection", image)
cv2.waitKey(0)
cv2.destroyAllWindows()


```

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://img-blog.csdnimg.cn/6567c240f1b5443ab60e194d3ffe3803.png)


ä»¥ä¸Šã€‚





